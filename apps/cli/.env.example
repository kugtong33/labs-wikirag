# CLI Environment Variables
# Copy this file to .env and fill in your values

# Embedding Provider Selection
# Options: openai | ollama
# Default: openai
EMBEDDING_PROVIDER=openai

# OpenAI API Key (required when EMBEDDING_PROVIDER=openai)
OPENAI_API_KEY=your-openai-api-key-here

# Qdrant Connection
QDRANT_URL=http://localhost:6333

# Local LLM (Ollama) — required when EMBEDDING_PROVIDER=ollama
# Start Ollama: docker compose up ollama   (GPU) or
#               docker compose -f docker-compose.yml -f docker-compose.cpu.yml up ollama  (CPU)
# Native install: curl -fsSL https://ollama.com/install.sh | sh && ollama serve
# Pull models:   ollama pull nomic-embed-text
#                ollama pull qwen3-embedding
# Available embedding models:
#   nomic-embed-text  — 274MB, 768 dims, strong English quality
#   qwen3-embedding   — 639MB–4.7GB, up to 4096 dims, #1 MTEB multilingual
OLLAMA_BASE_URL=http://localhost:11434
